{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88d62b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "# Available in the github repo : examples/data/BTC_USD-Hourly.csv\n",
    "url = \"https://raw.githubusercontent.com/ClementPerroud/Gym-Trading-Env/main/examples/data/BTC_USD-Hourly.csv\"\n",
    "df = pd.read_csv(url, parse_dates=[\"date\"], index_col= \"date\")\n",
    "df.sort_index(inplace= True)\n",
    "df.dropna(inplace= True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02728b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function library for indicators\n",
    "def sma(data, period):\n",
    "    return data.rolling(window=period).mean()\n",
    "\n",
    "def ema(data, period):\n",
    "    return data.ewm(span=period, min_periods=period).mean()\n",
    "\n",
    "def rsi(data, period):\n",
    "    delta = data.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    rsi = 100 - 100 / (1 + up.ewm(span=period, min_periods=period).mean() / down.ewm(span=period, min_periods=period).mean())\n",
    "    return rsi\n",
    "\n",
    "def macd(data, fast_period, slow_period, signal_period):\n",
    "    ema_fast = ema(data, fast_period)\n",
    "    ema_slow = ema(data, slow_period)\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal = ema(macd, signal_period)\n",
    "    return macd, signal\n",
    "\n",
    "def bollinger_bands(data, period):\n",
    "    std = data.rolling(window=period).std()\n",
    "    upper = ema(data, period) + 2 * std\n",
    "    lower = ema(data, period) - 2 * std\n",
    "    return upper, lower\n",
    "\n",
    "def atr(data, period):\n",
    "    high_low = data[\"high\"] - data[\"low\"]\n",
    "    close_prev_close = abs(data[\"close\"] - data[\"close\"].shift(1))\n",
    "    tr = pd.concat([high_low, close_prev_close], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=period).mean()\n",
    "    return atr\n",
    "\n",
    "def stochastic_oscillator(data, k_period, d_period):\n",
    "    low_k = data[\"low\"].rolling(window=k_period).min()\n",
    "    high_k = data[\"high\"].rolling(window=k_period).max()\n",
    "    k = 100 * (data[\"close\"] - low_k) / (high_k - low_k)\n",
    "    d = k.ewm(span=d_period, min_periods=d_period).mean()\n",
    "    return k, d\n",
    "\n",
    "def volume_ratio(data, period):\n",
    "    return data[\"volume\"].rolling(window=period).mean() / data[\"volume\"]\n",
    "\n",
    "# Add indicators with different measurements\n",
    "df[\"feature_SMA_10\"] = sma(df[\"close\"], 10)\n",
    "df[\"feature_SMA_50\"] = sma(df[\"close\"], 50)\n",
    "df[\"feature_EMA_20\"] = ema(df[\"close\"], 20)\n",
    "df[\"feature_RSI_7\"] = rsi(df[\"close\"], 7)\n",
    "df[\"feature_RSI_14\"] = rsi(df[\"close\"], 14)\n",
    "df[\"feature_MACD_12_26_9\"] = macd(df[\"close\"], 12, 26, 9)[0]\n",
    "df[\"feature_MACD_signal_9\"] = macd(df[\"close\"], 12, 26, 9)[1]\n",
    "df[\"feature_Bollinger_Bands_Upper_20\"] = bollinger_bands(df[\"close\"], 20)[0]\n",
    "df[\"feature_Bollinger_Bands_Lower_20\"] = bollinger_bands(df[\"close\"], 20)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0af1e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is a DataFrame with columns : \"open\", \"high\", \"low\", \"close\", \"Volume USD\"\n",
    "\n",
    "# Create the feature : ( close[t] - close[t-1] )/ close[t-1]\n",
    "df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "\n",
    "# Create the feature : open[t] / close[t]\n",
    "df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "\n",
    "# Create the feature : high[t] / close[t]\n",
    "df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "\n",
    "# Create the feature : low[t] / close[t]\n",
    "df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "\n",
    " # Create the feature : volume[t] / max(*volume[t-7*24:t+1])\n",
    "df[\"feature_volume\"] = df[\"Volume USD\"] / df[\"Volume USD\"].rolling(7*24).max()\n",
    "\n",
    "df.dropna(inplace= True) # Clean again !\n",
    "# Eatch step, the environment will return 5 inputs  : \"feature_close\", \"feature_open\", \"feature_high\", \"feature_low\", \"feature_volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4ac1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import environments\n",
    "\n",
    "#from environments import TradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b38268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_trading_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5111845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gymnasium as gym\n",
    "#import gym_trading_env\n",
    "#env = gym.make('MultiDatasetTradingEnv',\n",
    "#    dataset_dir = 'preprocessed_data/*.pkl',\n",
    "#    positions=[-1, 0, 1],  # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1b81501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 episodes\n",
    "#for _ in range(10):\n",
    "  # At every episode, the env will pick a new dataset.\n",
    "#  done, truncated = False, False\n",
    "#  observation, info = env.reset()\n",
    "#  while not done and not truncated:\n",
    "#      position_index = env.action_space.sample() # Pick random position index\n",
    "#      observation, reward, done, truncated, info = env.step(position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "336f33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import TradingEnv\n",
    "\n",
    "# Create the environment directly\n",
    "env = TradingEnv(\n",
    "    name=\"BTCUSD\",\n",
    "    df=df,  # Your dataset with your custom features\n",
    "    positions=[-0.9, 0.0, 0.9],  # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "    trading_fees=0.01/100,  # 0.01% per stock buy/sell (Binance fees)\n",
    "    borrow_interest_rate=0.0003/100,  # 0.0003% per timestep (one timestep = 1h here)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3986c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da903ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = \"MlpPolicy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4798e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(policy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f32869d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different hyperparameters based on your problem and environment\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.01,  # Adjust as needed\n",
    "    batch_size=32,  # Adjust based on memory and learning speed\n",
    "    gamma=0.5,  # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a5b2d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1505 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 892         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032951437 |\n",
      "|    clip_fraction        | 0.53        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -4.17       |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.00538     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.0236      |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046235498 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0265      |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 749         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024312086 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04460737 |\n",
      "|    clip_fraction        | 0.485      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.981     |\n",
      "|    explained_variance   | -0.00297   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0228     |\n",
      "|    value_loss           | 0.00277    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 718         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032878384 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0188      |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 713         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039625935 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0202      |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 703        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06602059 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.555     |\n",
      "|    explained_variance   | 0.00246    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.0159     |\n",
      "|    value_loss           | 0.00244    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043574855 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.00347     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 686        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12183027 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 0.006      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0127     |\n",
      "|    value_loss           | 0.000391   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 680        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06128861 |\n",
      "|    clip_fraction        | 0.0248     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0607    |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 0.0275     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | 0.00392    |\n",
      "|    value_loss           | 0.000102   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 671         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011774587 |\n",
      "|    clip_fraction        | 0.0041      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0102     |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 3.23e-05    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.000556    |\n",
      "|    value_loss           | 0.000519    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    316\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    317\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    319\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    320\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    321\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:299\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    301\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:279\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 279\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    281\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f91fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_trading_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a880e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment directly\n",
    "env2 = TradingEnv(\n",
    "    name=\"BTCUSD\",\n",
    "    df=df,  # Your dataset with your custom features\n",
    "    positions=[-0.9, 0, 0.9],  # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "    trading_fees=0.01/100,  # 0.01% per stock buy/sell (Binance fees)\n",
    "    borrow_interest_rate=0.0003/100,  # 0.0003% per timestep (one timestep = 1h here)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "obs, info = env2.reset()\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env2.step(action)\n",
    "    # ... (evaluate further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22913d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of the episode you want to render\n",
    "env2.unwrapped.save_for_render(dir=\"render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311fa00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
