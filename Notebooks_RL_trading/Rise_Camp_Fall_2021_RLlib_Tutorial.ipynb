{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_IXCSrDJacm"
      },
      "source": [
        "# Hands-on RL with Ray’s RLlib (Simplified Tutorial)\n",
        "<hr />\n",
        "\n",
        "## Tutorial for working with multi-agent environments, models, and algorithms\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1s1chO-ET7inBCKDdKgP4hI0UgTI4bLPs\" width=250> <img src=\"https://drive.google.com/uc?export=view&id=1GGD7V_oO1osZqgKF8QzajM3_bs5o9fNw\" width=169> <img src=\"https://drive.google.com/uc?export=view&id=1xJTlXqv182zVvDPeRc2lEg06zU0GbNrK\" width=252> <img src=\"https://drive.google.com/uc?export=view&id=1X3eVsp3hhFzwFaeqOwwZ9DmJ0UiYfu4y\" width=213>\n",
        "\n",
        "### Overview\n",
        "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
        "\n",
        "### Intended Audience\n",
        "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
        "\n",
        "### Prerequisites\n",
        "* Some Python programming experience.\n",
        "* Some familiarity with machine learning.\n",
        "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
        "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
        "\n",
        "### Requirements/Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "l93OCdB-JZHF",
        "outputId": "8439a9d3-28d5-4f92-bb10-dde15f0f62b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ray[rllib]\n",
            "  Downloading ray-1.8.0-cp37-cp37m-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (21.2.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.41.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.13)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (2.6.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.17.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 735 kB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.1.6)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.16.2)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 73.1 MB/s \n",
            "\u001b[?25hCollecting lz4\n",
            "  Downloading lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.8.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[rllib]) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (0.10.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->ray[rllib]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ray[rllib]) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[rllib]) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2021.5.30)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.4.1)\n",
            "Installing collected packages: redis, tensorboardX, ray, lz4\n",
            "Successfully installed lz4-3.1.3 ray-1.8.0 redis-3.5.3 tensorboardX-2.4\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Collecting libclang>=9.0.1\n",
            "  Downloading libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 246 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 87.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-estimator, libclang, keras, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.6.0\n",
            "    Uninstalling keras-2.6.0:\n",
            "      Successfully uninstalled keras-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "Successfully installed keras-2.7.0 libclang-12.0.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.21.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ray[rllib]\n",
        "!pip install tensorflow -U  # <- either one works!\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyoT5cyVKYa_"
      },
      "source": [
        "### Key Takeaways\n",
        "* What is reinforcement learning and why RLlib?\n",
        "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
        "\n",
        "### Tutorial Outline (30-40 min)\n",
        "1. RL and RLlib in a nutshell.\n",
        "1. Defining an RL-solvable problem: Our first (multi-agent) environment.\n",
        "1. **Exercise No.1**: Environment Loop.\n",
        "1. Picking an algorithm and training our first RLlib Trainer.\n",
        "1. **Exercise No.2** Fixing our experiment's config - Going multi-agent.\n",
        "\n",
        "### Other Recommended Readings\n",
        "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mgu5vPHwTB-3uch1d43BICQoK0h9XkbO\" width=400>\n",
        "\n",
        "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
        "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_44DB84Sgwd"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "### Coding/defining our \"problem\" via an RL environment.\n",
        "\n",
        "We will use the following (adversarial) multi-agent environment\n",
        "throughout this tutorial to demonstrate RLlib's\n",
        "APIs, features, and customization options.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GL5LDrrnw0rx-cYK9ucQ4drpaykz1pBd\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wlIyZPBTWgf"
      },
      "source": [
        "### A word or two on Spaces:\n",
        "\n",
        "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
        "\n",
        "RL environments also use them to describe what their valid observations and actions are.\n",
        "\n",
        "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
        "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
        "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be `Discrete(4)`\n",
        "(no datatype, no shape needs to be defined here). Our observation space will be `MultiDiscrete([n, m])`, where n is the position of the agent observing and m is the position of the opposing agent, so if agent1 starts in the upper left corner and agent2 starts in the bottom right corner, agent1's observation would be: `[0, 63]` (in an 8 x 8 grid) and agent2's observation would be `[63, 0]`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1zTklLKfSzK4ia054NNFMq3KLWii2QYa3\" width=800>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-SaHGPCzPa5"
      },
      "outputs": [],
      "source": [
        "# Let's code our multi-agent environment.\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Discrete, MultiDiscrete\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "\n",
        "class MultiAgentArena(MultiAgentEnv):\n",
        "    def __init__(self, config=None):\n",
        "        \"\"\" Config takes in width, height, and ts \"\"\"\n",
        "        config = config or {}\n",
        "        # Dimensions of the grid.\n",
        "        self.width = config.get(\"width\", 10)\n",
        "        self.height = config.get(\"height\", 10)\n",
        "\n",
        "        # End an episode after this many timesteps.\n",
        "        self.timestep_limit = config.get(\"ts\", 100)\n",
        "\n",
        "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
        "                                                self.width * self.height])\n",
        "        # 0=up, 1=right, 2=down, 3=left.\n",
        "        self.action_space = Discrete(4)\n",
        "\n",
        "        # Reset env.\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
        "        # Row-major coords.\n",
        "        self.agent1_pos = [0, 0]  # upper left corner\n",
        "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
        "\n",
        "        # Accumulated rewards in this episode.\n",
        "        self.agent1_R = 0.0\n",
        "        self.agent2_R = 0.0\n",
        "\n",
        "        # Reset agent1's visited fields.\n",
        "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
        "\n",
        "        # How many timesteps have we done in this episode.\n",
        "        self.timesteps = 0\n",
        "\n",
        "        # Return the initial observation in the new episode.\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action: dict):\n",
        "        \"\"\"\n",
        "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
        "\n",
        "        e.g.\n",
        "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
        "        \"\"\"\n",
        "\n",
        "        # increase our time steps counter by 1.\n",
        "        self.timesteps += 1\n",
        "        # An episode is \"done\" when we reach the time step limit.\n",
        "        is_done = self.timesteps >= self.timestep_limit\n",
        "\n",
        "        # Agent2 always moves first.\n",
        "        # events = [collision|agent1_new_field]\n",
        "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
        "        events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
        "\n",
        "        # Useful for rendering.\n",
        "        self.collision = \"collision\" in events\n",
        "\n",
        "        # Get observations (based on new agent positions).\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        # Determine rewards based on the collected events:\n",
        "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
        "        r2 = 1.0 if \"collision\" in events else -0.1\n",
        "\n",
        "        self.agent1_R += r1\n",
        "        self.agent2_R += r2\n",
        "\n",
        "        rewards = {\n",
        "            \"agent1\": r1,\n",
        "            \"agent2\": r2,\n",
        "        }\n",
        "\n",
        "        # Generate a `done` dict (per-agent and total).\n",
        "        dones = {\n",
        "            \"agent1\": is_done,\n",
        "            \"agent2\": is_done,\n",
        "            # special `__all__` key indicates that the episode is done for all agents.\n",
        "            \"__all__\": is_done,\n",
        "        }\n",
        "\n",
        "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
        "        agent's current x/y-positions.\n",
        "        \"\"\"\n",
        "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
        "            (self.agent1_pos[1] % self.width)\n",
        "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
        "            (self.agent2_pos[1] % self.width)\n",
        "        return {\n",
        "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
        "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
        "        }\n",
        "\n",
        "    def _move(self, coords, action, is_agent1):\n",
        "        \"\"\"\n",
        "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
        "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
        "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
        "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
        "        \"\"\"\n",
        "        orig_coords = coords[:]\n",
        "        # Change the row: 0=up (-1), 2=down (+1)\n",
        "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
        "        # Change the column: 1=right (+1), 3=left (-1)\n",
        "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
        "\n",
        "        # Solve collisions.\n",
        "        # Make sure, we don't end up on the other agent's position.\n",
        "        # If yes, don't move (we are blocked).\n",
        "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
        "            coords[0], coords[1] = orig_coords\n",
        "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
        "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
        "            return {\"collision\"}\n",
        "\n",
        "        # No agent blocking -> check walls.\n",
        "        if coords[0] < 0:\n",
        "            coords[0] = 0\n",
        "        elif coords[0] >= self.height:\n",
        "            coords[0] = self.height - 1\n",
        "        if coords[1] < 0:\n",
        "            coords[1] = 0\n",
        "        elif coords[1] >= self.width:\n",
        "            coords[1] = self.width - 1\n",
        "\n",
        "        # If agent1 -> \"new\" if new tile covered.\n",
        "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
        "            self.agent1_visited_fields.add(tuple(coords))\n",
        "            return {\"agent1_new_field\"}\n",
        "        # No new tile for agent1.\n",
        "        return set()\n",
        "\n",
        "    def render(self, mode=None):\n",
        "        print(\"_\" * (self.width + 2))\n",
        "        for r in range(self.height):\n",
        "            print(\"|\", end=\"\")\n",
        "            for c in range(self.width):\n",
        "                field = r * self.width + c % self.width\n",
        "                if self.agent1_pos == [r, c]:\n",
        "                    print(\"1\", end=\"\")\n",
        "                elif self.agent2_pos == [r, c]:\n",
        "                    print(\"2\", end=\"\")\n",
        "                elif (r, c) in self.agent1_visited_fields:\n",
        "                    print(\".\", end=\"\")\n",
        "                else:\n",
        "                    print(\" \", end=\"\")\n",
        "            print(\"|\")\n",
        "        print(\"‾\" * (self.width + 2))\n",
        "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
        "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
        "        print(\"R2={: .1f}\".format(self.agent2_R))\n",
        "        print()\n",
        "\n",
        "\n",
        "env = MultiAgentArena()\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "# Agent1 will move down, Agent2 moves up.\n",
        "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
        "\n",
        "env.render()\n",
        "\n",
        "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
        "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
        "print(\"Env timesteps={}\".format(env.timesteps))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ler482dUPUbn"
      },
      "source": [
        "## Exercise No 1: Environment Rollout\n",
        "\n",
        "<hr />\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Ta1s0QOfSCtuK0ZbmviwkI_6GcBWmXzY\" width=800>\n",
        "\n",
        "In the cell above, we performed a `reset()` and a single `step()` call. To walk through an entire episode, one would normally call `step()` repeatedly (with different actions) until the returned `done` dict has the \"agent1\" or \"agent2\" (or \"__all__\") key set to True. Your task is to write an \"environment loop\" that runs for exactly one episode using our `MultiAgentArena` class.\n",
        "\n",
        "Follow these instructions here to get this done.\n",
        "\n",
        "1. `reset` the already created (variable `env`) environment to get the first (initial) observation.\n",
        "1. Enter an infinite while loop.\n",
        "1. Compute the actions for \"agent1\" and \"agent2\" calling `DummyTrainer.compute_action([obs])` twice (once for each agent).\n",
        "1. Put the results of the action computations into an action dict (`{\"agent1\": ..., \"agent2\": ...}`).\n",
        "1. Pass this action dict into the env's `step()` method, just like it's done in the above cell (where we do a single `step()`).\n",
        "1. Check the returned `dones` dict for True (yes, episode is terminated) and if True, break out of the loop.\n",
        "\n",
        "**Good luck! :)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QjeiraIPVIv"
      },
      "outputs": [],
      "source": [
        "class DummyTrainer:\n",
        "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
        "\n",
        "    Use its `compute_action` method to get a new action for one of the agents,\n",
        "    given the agent's observation (a single discrete value encoding the field\n",
        "    the agent is currently in).\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_action(self, single_agent_obs=None):\n",
        "        # Returns a random action for a single agent.\n",
        "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
        "\n",
        "dummy_trainer = DummyTrainer()\n",
        "# Check, whether it's working.\n",
        "for _ in range(3):\n",
        "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
        "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
        "\n",
        "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
        "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f2cBNb5z39j"
      },
      "outputs": [],
      "source": [
        "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
        "import time\n",
        "from ipywidgets import Output\n",
        "from IPython import display\n",
        "import time\n",
        "\n",
        "out = Output()\n",
        "display.display(out)\n",
        "\n",
        "with out:\n",
        "\n",
        "    # Exercise #1:\n",
        "\n",
        "    # Start coding here inside this `with`-block:\n",
        "    # 1) Reset the env.\n",
        "\n",
        "    # 2) Enter an infinite while loop (to step through the episode).\n",
        "\n",
        "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])\n",
        "\n",
        "        # 4) Compile the actions dict from both individual agents' actions.\n",
        "\n",
        "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts\n",
        "\n",
        "        # 6) We'll do this together: Render the env.\n",
        "        # Don't write any code here (skip directly to 7).\n",
        "        out.clear_output(wait=True)\n",
        "        time.sleep(0.08)\n",
        "        env.render()\n",
        "\n",
        "        # 7) Check, whether the episde is done, if yes, break out of the while loop.\n",
        "\n",
        "# 8) Run it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_3_s1f3HW0"
      },
      "source": [
        "## Training with RLlib's PPO\n",
        "\n",
        "We will now train an RL agent with RLlib's PPO. PPO is well-known in the RL community to be one of the most reliable algorithms that works most classes of environments.\n",
        "\n",
        "There are many different algos in RLlib (over 20!) and you can mix match whatever algorithm you like to train your RL agent. This is what makes RLlib a versatile library to use!\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11pv431GA0frNFZIRfeSp0mMeJ2coTkPW\" width=800>\n",
        "\n",
        "\n",
        "### Initializing Ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q99aq1iV3E5q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "import ray\n",
        "\n",
        "# Start a new instance of Ray (when running this tutorial locally) or\n",
        "# connect to an already running one (when running this tutorial through Anyscale).\n",
        "# ray.shutdown()\n",
        "ray.init()  # Hear the engine humming? ;)\n",
        "\n",
        "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
        "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpL4Muv58zP8"
      },
      "source": [
        "### Creating an RLlib Trainer (PPOTrainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OQ67lb35c-y"
      },
      "outputs": [],
      "source": [
        "# Import a Trainable (one of RLlib's built-in algorithms):\n",
        "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
        "# action spaces and model types and b/c it learns well almost any problem.\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Specify a very simple config, defining our environment and some environment\n",
        "# options (see environment.py).\n",
        "config = {\n",
        "    \"env\": MultiAgentArena,\n",
        "    \"env_config\": {\n",
        "        \"config\": {\n",
        "            \"width\": 10,\n",
        "            \"height\": 10,\n",
        "            \"ts\": 100,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # !PyTorch users!\n",
        "    \"framework\": \"tf\",  # If users have chosen to install torch instead of tf.\n",
        "\n",
        "    \"create_env_on_driver\": True,\n",
        "}\n",
        "# Instantiate the Trainer object using above config.\n",
        "rllib_trainer = PPOTrainer(config=config)\n",
        "rllib_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udmcMZwI9lkW"
      },
      "source": [
        "### Ready to train with RLlib's PPO algorithm\n",
        "\n",
        "That's it, we are ready to train.\n",
        "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
        "\n",
        "One iteration for most algos involves:\n",
        "\n",
        "1) sampling from the environment(s)\n",
        "\n",
        "2) using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
        "\n",
        "Let's try it out!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVUIlCyU5xA6"
      },
      "outputs": [],
      "source": [
        "# Runs 1 Iteration of Training\n",
        "results = rllib_trainer.train()\n",
        "\n",
        "# Delete the config from the results for clarity.\n",
        "# Only the stats will remain, then.\n",
        "del results[\"config\"]\n",
        "# Pretty print the stats.\n",
        "pprint.pprint(results)\n",
        "del rllib_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-0L4KcJAu2B"
      },
      "source": [
        "## Exercise 2: Training with Multiple Policies\n",
        "\n",
        "So far, our experiment has been ill-configured, because both\n",
        "agents, which should behave differently due to their different\n",
        "tasks and reward functions, learn the same policy: the \"default_policy\",\n",
        "which RLlib always provides if you don't configure anything else.\n",
        "\n",
        "Remember that RLlib does not know at Trainer setup time, how many and which agents the environment will \"produce\". Agent control (adding agents, removing them, terminating episodes for agents) is entirely in the Env's hands.\n",
        "Let's fix our single policy problem and introduce the \"multiagent\" API.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rsRMLN8KyEHKS4XCcjRmUW19kpRjqB8z\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml_yZPxqBri1"
      },
      "source": [
        "In order to turn on RLlib's multi-agent functionality, follow these instructions:\n",
        "\n",
        "1. A policies definition dict, mapping policy IDs (e.g. \"policy1\") to 4-tuples consisting of 1) policy class (None for using the default class), 2) observation space, 3) action space, and 4) config overrides (empty dict for no overrides and using the Trainer's main config dict).\n",
        "1. A policy mapping function, mapping agent IDs (e.g. a string like \"agent1\", produced by the environment in the returned observation/rewards/dones-dicts) to a policy ID (another string, e.g. \"policy1\").\n",
        "1. Pass in the policy mapping function and policy configs into the Trainer config.\n",
        "1. Train!\n",
        "\n",
        "If stucked, https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical provides a great example.\n",
        "\n",
        "**Good luck! :)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH_QP5MbN8Iz"
      },
      "outputs": [],
      "source": [
        "# Run this if neccessary\n",
        "ray.shutdown()\n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFeuLIe26CZ4"
      },
      "outputs": [],
      "source": [
        "# Exercise 2\n",
        "# 1) Define the policies definition dict:\n",
        "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
        "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
        "# - obs-space (we get this directly from our already created env object).\n",
        "# - act-space (we get this directly from our already created env object).\n",
        "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
        "policies = {\n",
        "    ### Modify Code here ####\n",
        "    \"policy1\": None,\n",
        "    \"policy2\": None,\n",
        "}\n",
        "# Note that now we won't have a \"default_policy\" anymore, just \"policy1\" and \"policy2\".\n",
        "\n",
        "# 2) Defines an agent->policy mapping function.\n",
        "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
        "def policy_mapping_fn(agent_id: str) -> str:\n",
        "    # Make sure agent ID is valid.\n",
        "    assert agent_id in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent ID {agent_id}!\"\n",
        "    ### Modify Code here ####\n",
        "    return None\n",
        "\n",
        "config = {\n",
        "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
        "    \"env_config\": {\n",
        "        \"config\": {\n",
        "            \"width\": 10,\n",
        "            \"height\": 10,\n",
        "            \"ts\": 100,\n",
        "        },\n",
        "    },\n",
        "    # !PyTorch users!\n",
        "    \"framework\": \"tf\",  # If users have chosen to install torch instead of tf.\n",
        "    \"create_env_on_driver\": True,\n",
        "}\n",
        "\n",
        "# 3) Adding the above to our config.\n",
        "### Modify Code here ####\n",
        "config.update({\n",
        "    \"multiagent\": {\n",
        "        \"policies\": None,\n",
        "        \"policy_mapping_fn\": None,\n",
        "    },\n",
        "})\n",
        "\n",
        "pprint.pprint(config)\n",
        "print()\n",
        "print(f\"agent1 is now mapped to {policy_mapping_fn('agent1')}\")\n",
        "print(f\"agent2 is now mapped to {policy_mapping_fn('agent2')}\")\n",
        "\n",
        "rllib_trainer = PPOTrainer(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS_HsAcTApSb"
      },
      "outputs": [],
      "source": [
        "# 4) Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
        "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
        "for _ in range(10):\n",
        "    ### Modify Code here ####\n",
        "    results = None\n",
        "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1OoY9xnFzhy"
      },
      "source": [
        "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ReNk_4F3EZ"
      },
      "outputs": [],
      "source": [
        "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
        "for _ in range(10):\n",
        "    results = rllib_trainer.train()\n",
        "    r1 = results['policy_reward_mean']['policy1']\n",
        "    r2 = results['policy_reward_mean']['policy2']\n",
        "    r = r1 + r2\n",
        "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={r} R1={r1} R2={r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ginSlspOoPX"
      },
      "source": [
        "## Evaluating Multiagent PPO Trainer\n",
        "\n",
        "Now that we are done training with PPO, let's evaluate how the agents behave, using our code in Exercise 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMnpjrxKF7b4"
      },
      "outputs": [],
      "source": [
        "out = Output()\n",
        "display.display(out)\n",
        "\n",
        "with out:\n",
        "    env = MultiAgentArena()\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        a1 = rllib_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
        "        a2 = rllib_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
        "        obs, rewards, dones, _ = env.step({\"agent1\": a1, \"agent2\": a2})\n",
        "        out.clear_output(wait=True)\n",
        "        time.sleep(0.08)\n",
        "        env.render()\n",
        "        if dones[\"agent1\"]:\n",
        "          break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85XForDFSEIo"
      },
      "source": [
        "## Time for Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fZqmYQ6WYxLI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}